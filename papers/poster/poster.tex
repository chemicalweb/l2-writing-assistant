\documentclass[final,t]{beamer}
\mode<presentation>{\usetheme{Purdue}}

\usepackage{natbib,url}
\usepackage{multicol}
\usepackage{geometry}
\usepackage{multirow}
\usepackage{rotating}

\usepackage{enumerate}
\usepackage{color}
\usepackage{xcolor}

\definecolor{light-gray}{gray}{0.9}

\usepackage[orientation=portrait,size=a1,scale=1]{beamerposter}
\usepackage{gb4e}

\title[]{IUCL: Combining Information Sources for SemEval Task 5}
\author[]{Alex Rudnick, Levi King, Can Liu, Markus Dickinson, Sandra K\"ubler}
\institute[]{Indiana University}
\date[]{XX August 2014}

\begin{document}
\begin{frame}{}
  \begin{columns}[t]
    \begin{column}{.4\linewidth}

\begin{block}{Approach}

  \textbf{Our approach:} make use of rich L2 context, along with
  proposed fragment translations

  \medskip

  \begin{enumerate}
  \item Model transfer knowledge between L1 \& L2: look up candidate
    translations for L1 fragment
  \item Model facts about L2 syntax: score candidate translations via
    L2 LM
  \item Model L2 collocational tendencies: score translations via
    dependency-driven word similarity (\textit{SIM})
  \item Weight each source of information (\#1--\#3) in log-linear
    model to obtain final $n$-best list
  \end{enumerate}

  $\Rightarrow$ Language-independent approach

\end{block}

\begin{block}{Data sources}

\begin{center}
\textcolor{purple}{Europarl}
\end{center}

\colorbox{white}{
\begin{minipage}{.85\linewidth}
\begin{itemize}
\item parallel corpus: proc.\ of European Parliament
\item used for L2 candidate generation
\item  used to generate phrase tables
\item small, clean, out of domain
\end{itemize}
\end{minipage}
}

\begin{center}
\textcolor{purple}{BabelNet}
\end{center}

\colorbox{white}{
\begin{minipage}{.85\linewidth}
\begin{itemize}
\item multilingual encyclopedic dictionary
\item used as L2 back-off 
\item look up translations for phrases \& sub-phrases
\end{itemize}
\end{minipage}
}

\begin{center}
\textcolor{purple}{Wikipedia}
\end{center}

\colorbox{white}{
\begin{minipage}{.85\linewidth}
\begin{itemize}
\item used for estimating collocational tendencies
\item size: 12-28M sentences
\item POS tagged \& dependency parsed (Stanford Parser)
\end{itemize}
\end{minipage}
}

\begin{center}
\textcolor{purple}{Google Books Syntactic N-grams}
\end{center}

\colorbox{white}{
\begin{minipage}{.85\linewidth}
\begin{itemize}
\item English only
\item additional training set for collocations tendencies
\item syntactic $n$-grams 

\end{itemize}
\end{minipage}
}

\end{block}

\end{column}

\begin{column}{.55\linewidth}

%\begin{columns}
%\begin{column}{.48\linewidth}

\begin{block}{System Design}

\begin{center}
  \textcolor{purple}{Constructing Candidate Translations}
 \end{center}

 \colorbox{white}{
\begin{minipage}{.90\linewidth}
 \begin{itemize}
  \item build phrase tables using Moses
  \item during translation: look up source lg.\ phrase, extract all translations from phrase table
  \item back-off 1: construct ``synthetic phrase'' from sub-phrases
  \item back-off 2: look up phrase in BabelNet; estimate probs uniformly
  \item back-off 3: look up phrase in Google Translate\\
  \end{itemize}
\end{minipage}
}
\vspace{1cm}


\begin{center}
  \textcolor{purple}{Scoring Candidate Translations via a L2 Language Model}
\end{center}
 
  \colorbox{white}{
\begin{minipage}{.90\linewidth}
 \begin{itemize}
  \item model the fit of phrase in context
  \item apply language model (KenLM); trained on Wikipedia data
  \end{itemize}
\end{minipage}
}
\vspace{1cm}

%\end{center}

%\end{block}

%\end{column}

%\begin{column}{.48\linewidth}

%\begin{block}{System Design}
%\begin{center}

\begin{center}
  \textcolor{purple}{Scoring Candidate Translations via Dependency-Based Word Similarity}
\end{center}

  \colorbox{white}{
\begin{minipage}{.90\linewidth}
  \begin{itemize}
  \item basic idea: use deeper, syntax-based information to estimate fit
  \item use adaptation of Lin's similarity metric
  \item similarity is summed over all words in translated phrase
  \item uses dependencies to \& from this word to context
  \item similarity based on head, dependent, and label of dependency 
  \item back-off: use POS instead of words
  \item example: \textcolor{blue}{The suitcases disappeared after the $<$Aufgabe$>$ at the airport.}\\
  	most frequent sense: \textcolor{blue}{Aufgabe = task}\\
	google translate: \textcolor{blue}{The suitcase had disappeared after posting at the airport.}
  \end{itemize}
\end{minipage}
}
\vspace{1cm}

\begin{center}
  \textcolor{purple}{Tuning Weights with MERT}
\end{center}

  \colorbox{white}{
\begin{minipage}{.90\linewidth}
  \begin{itemize}
  \item use long-linear model to combine scores from different types of information
  \item weights for different types of information estimated using ZMERT
  \item ZMERT optimized for BLEU scores
  \end{itemize}
\end{minipage}
}


\end{block}

    \end{column}
%  \end{columns}
%  \end{column}
  \end{columns}

\begin{columns}
  \begin{column}{.64\linewidth}

\begin{block}{Results}

\begin{center}
  \textcolor{purple}{English-German (next-best: CNRC-run1)}
  %\begin{figure}[t]
  \begin{center}
  \begin{tabular}{|r|c|c|c|c|}
    \hline
    system & acc      & wordacc  & oofacc & oofwordacc \\
    \hline
    run2  &  0.665 & 0.722  &  0.806  & 0.857 \\
    SIM    &  0.647 & 0.706 & 0.800 & 0.852 \\
    next-best     &  0.657   & 0.717   & 0.834 & 0.868    \\
    \hline
  \end{tabular}
  \end{center}
%\caption{Scores on the test set for English-German; here next-best is CNRC-run1.}
%\label{fig:theresults-en-de}
%\end{figure}

  \textcolor{purple}{English-Spanish (best: UEdin-run2)}
%\begin{figure}[t]
  \begin{center}
  \begin{tabular}{|r|c|c|c|c|}
    \hline
    system & acc      & wordacc  & oofacc & oofwordacc \\
    \hline
    run2  &  0.633 & 0.72 & 0.781 & 0.847 \\
    SIM    &  0.359 &  0.482 & 0.462 & 0.607 \\
    best   &  0.755 & 0.827   & 0.920  & 0.944 \\
    \hline
  \end{tabular}
  \end{center}
%\caption{Scores on the test set for English-Spanish; here best is UEdin-run2.}
%\label{fig:theresults-en-es}
%\end{figure}

 \textcolor{purple}{French-English (best: UEdin-run1)}
%\begin{figure}[t]
  \begin{center}
  \begin{tabular}{|r|c|c|c|c|}
    \hline
    system & acc      & wordacc  & oofacc & oofwordacc \\
    \hline
    run2  & 0.545  & 0.682 & 0.691 & 0.800 \\
    SIM        &  0.549 & 0.687 & 0.693 & 0.800 \\
    best & 0.733 & 0.824 & 0.905 & 0.938 \\
    \hline
  \end{tabular}
  \end{center}
%\caption{Scores on the test set for French-English; here best is UEdin-run1.}
%\label{fig:theresults-fr-en}
%\end{figure}

  \textcolor{purple}{Dutch-English (best: UEdin-run1)}
%\begin{figure}[t]
  \begin{center}
  \begin{tabular}{|r|c|c|c|c|}
    \hline
    system & acc      & wordacc  & oofacc & oofwordacc \\
    \hline
    run2        &  0.544      &  0.679  & 0.634   & 0.753    \\
    SIM              &  0.540      &  0.676  & 0.635   & 0.753    \\
    best       &  0.575   & 0.692  & 0.733      &  0.811 \\
    \hline
  \end{tabular}
  \end{center}
%\caption{Scores on the test set for Dutch-English; here best is UEdin-run1.}
%\label{fig:theresults-nl-en}
%\end{figure}

\end{center}
\end{block}

  \end{column}

  \begin{column}{.31\linewidth}

\begin{block}{Conclusion}

\begin{itemize}
\item L2 language assistant based on language models
\item approach is language independent
\item uses syntactic information
\item but: syntactic information currently not helpful 
\end{itemize}

\end{block}

  \end{column}
\end{columns}

\end{frame}
\end{document}