\documentclass[11pt]{article}
\usepackage{semeval2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym,amsmath}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand{\wsname}{SemEval-2014}
\newcommand{\submissionpage}{\url{http://alt.qcri.org/semeval2014/index.php?id=cfp}}
\newcommand{\filename}{semeval2014}
\newcommand{\contact}{pnakov qf.org.qa}

\title{IUCL: Combining Information Sources for SemEval Task 5}

\author{Alex Rudnick, Levi King, Can Liu, Markus Dickinson, Sandra K\"ubler  \\
  Indiana University \\
  Bloomington, IN, USA \\
  {\tt \{alexr,leviking,liucan,md7,skuebler\}@indiana.edu} }

\date{}

\begin{document}
\maketitle
\begin{abstract}
Here we describe the Indiana University system for SemEval Task 5, the L2
writing assistant task, as well as some extensions to the system that were
completed after the main evaluation. Our team submitted translations for all
four language pairs in the evaluation, yielding the top scores for
English-German.  The system is based on combining different information sources
to arrive at a final L2 translation for a given L1 text fragment, incorporating
phrase tables extracted from bitexts, an L2 language model, a multilingual
dictionary, and dependency-based collocational models derived from large
samples of target-language text.
\end{abstract}


\section{Introduction}

\blfootnote{
    %
    % for review submission
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % Place licence statement here for the camera-ready version, see
    % Section~\ref{licence} of the instructions for preparing a
    % manuscript.
    %
    % % final paper: en-uk version (to license, a licence)
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % This work is licensed under a Creative Commons 
    % Attribution 4.0 International Licence.
    % Page numbers and proceedings footer are added by
    % the organisers.
    % Licence details:
    % \url{http://creativecommons.org/licenses/by/4.0/}
    % 
    % % final paper: en-us version (to licence, a license)
    %
    \hspace{-0.65cm}  % space normally used by the marker
    This work is licenced under a Creative Commons 
    Attribution 4.0 International License.
    Page numbers and proceedings footer are added by
    the organizers.
    License details:
    \url{http://creativecommons.org/licenses/by/4.0/}
}

The task of translating an L1 fragment occurring in the midst of an L2 sentence is one in which a phrase occurs in an already-rich target language (L2) context; this makes the task quite different from translation in the general case. For this reason, we decided to approach the problem by combining standard Machine Translation technology with target language information,
% allows one to rely more heavily on target information, 
such as contextual relationships. 
% We use a log-linear model ... 
This is broken down into various steps: 1) constructing candidate translations for the L1 fragment, including weights for the likelihood of each translation; 2) scoring candidate translations via a language model of the L2; 3) scoring candidate translations via dependency-driven word similarity measure \cite{lin:98} (which we call \textit{SIM})
%pointwise mutual information (PMI); 
and 4) combining the scores from 1)-3) via minimized error rate training (MERT), to arrive at a final solution.  Step 1) models transfer knowledge between the L1 and L2; step 2) models facts about the L2 grammar, i.e., what translations fit well into the local context; step 3) models collocational and semantic tendencies of the L2; and step 4)  gives different weights to each of the three sources of information.  Although we did not finish step 3) in time for the official results, we report it here, as it represents the most novel aspect of the system -- namely, the exploitation of the rich L2 context -- and it results in our team's best system.  In general, our system is fully language-independent, with accuracy varying due to the size of data sources and quality of input technology (e.g., syntactic parse accuracy).

\section{Data Sources}
%%%% CL: added the following, not sure if correct.
The data sources serve two major steps of our system: for L2 candidate generation, we use Europarl and BabelNet; and for candidate ranking using L2 context, we use Wikipedia and the Google Books Syntactic N-grams.  For the candidate generation, we later experiment with expanding to larger sets of data (section~\ref{sec:exp:large-pts}).
%The data sources are described below:\\

\paragraph{Europarl}  %(MD: for phrase alignment / candidate generation?)
%%%% CL: Added the following sentence not sure it is correct.
The Europarl Parallel Corpus (Europarl, v7) \cite{koehn:05} is a corpus of proceedings of the European Parliament, containing 21 European languages with sentence alignments.
%It is a good resource for constructing translation phrase tables because the corpus contains 21 European Languages, and it is aligned sentence by sentence. 
From this corpus, we build phrase tables for English-Spanish, English-German, French-English, Dutch-English.
%%% CL: Alex, is that right?

\paragraph{BabelNet} %(MD: can’t remember what this was for …)
%%% CL: added not sure if it correct. 
\marginpar{MD: check version number}

In the cases where the constructed phrase tables do not contain a translation
for a source phrase, we will need to back off to smaller phrases and find
candidate translations for these components.  To better handle sparsity, we
extend look-up using the multilingual dictionary BabelNet, v2.5
\cite{Navigli:Ponzetto:12} as a way to look up translation candidates.

\paragraph{Wikipedia} %(MD: for dependency/PMI calculation?) Levi:

%- we used: University of Pisa wikipedia extractor

For German and Spanish, we use recent Wikipedia dumps, obtained through the Wikipedia Extractor tool.\footnote{\url{http://medialab.di.unipi.it/wiki/Wikipedia_Extractor}}\marginpar{LK: Alex may have the exact info for the dumps if needed}. To save time during parsing, sentences longer than 25 words are removed. The remaining sentences are POS-tagged and dependency parsed using Mate Parser with its pre-trained models \cite{bohnet:10,bohnet:kuhn:12,seeker:kuhn:13}. To keep our English Wikipedia data set comparable in size to the German and Spanish sets, we choose an older (2006), smaller dump. Long sentences were removed, and the remaining sentences are POS-tagged and dependency parsed using the pre-trained Stanford Parser \cite{klein:manning:03,marneffe:maccartney:ea:06}. The resulting sizes of the data sets are (roughly): German: 389M words, 28M sentences; Spanish: 147M words, 12M sentences; English: 253M words, 15M sentences. Dependencies extracted from these parsed data sets serve as training for the SIM system described in section \ref{sec:dependencySIM}.
%%% CL: training changed to data source, because it is not strictly training?

\paragraph{Google Books Syntactic N-grams}
% \marginpar{\cite{goldberg:orwant:13} says this contains 919M ``items''; = dependency types? Taken from 3,473,595 books.}
For English, we also trained a SIM system on the arcs dataset of the Google Books Syntactic N-Grams \cite{goldberg:orwant:13}, which has 919M items.%[[
%--Levi: Here are more exact numbers, if needed:
%German Wikipedia:
%number_of_sents: 27,657,877
%number_of_words: 388,635,816
%avg wds per sent: 14.05
%Spanish Wikipedia:
%number_of_sents: 11,577,130
%number_of_words: 146,850,355
%avg wds per sent: 12.68
%English Wikipedia:
%number_of_sents: 14,547,754
%number_of_words: 252,934,861
%avg wds per sent: 17.39
%]]

\section{Our  System}
\label{sec:system}

As previously mentioned, at run-time our system decomposes the fragment
translation task into two parts: generating a number of possible candidate
translations, and then scoring and ranking them in the target-language context,
such that best-scoring translations can be returned.

\subsection{Constructing Candidate Translations}
\label{sec:candidates}

As a starting point, we use phrase tables constructed in a fairly standard way,
using the training scripts packaged with Moses \cite{koehn:hoang:ea:07}, which
preprocess the given bitext, find word alignments with GIZA++ \cite{och:ney:00}
and then extract phrases with the \texttt{grow-diag-final-and} heuristic. 

At translation time, we look for the given source-language phrase in the phrase
table, and if it is found, we take all translations of that phrase as our
candidates.

When trying to translate a source-language phrase that is not found in the
phrase table, we try to construct a ``synthetic phrase" out of the available
components. This is done by listing, combinatorially, all ways to decompose the
input phrase into sub-phrases of at least one token long. Then for each
decomposition of the input phrase such that all of its components can be found
in the phrase table, we generate a translation by concatenating their
target-language sides. This approach naively assumes that generating valid L2
text requires no reordering of the components. Also, since there are $2^{n-1}$
possible ways to split an $n$-token phrase into sub-sequences (\textit{i.e.},
each token is either the first token in a new sub-sequence, or it is not), we
perform some heuristic pruning at this step, taking only the first 100
decompositions, preferring those built from longer phrase-table entries. Every
phrase in the phrase table, including these synthetic phrases, has both a
``direct" and ``inverse" probability score; for synthetic phrases, we produce
probability estimates by multiplying the scores of each of its components.

In the case that an individual word cannot be found in the phrase table, the
system attempts to look up the word in BabelNet, estimating the probabilities
as uniformly distributed over the available entries in BabelNet. Thus,
synthetic phrase table entries can be constructed by combining phrases found in
the training data and words available in BabelNet.

For the evaluation, in cases where an L1 phrase contained words that were
neither in our training data nor BabelNet (and thus were simply
out-of-vocabulary for our system), we took the first translation for that
phrase, without regard to context, from Google Translate, through the
semi-automated Google Docs interface. This 

\begin{itemize}
\item some text: split up the source phrase into smaller components
  and look up each components from BabelNet.
\item MD: 1. What is the exact back-off procedure? 2. Do we back off
  only in the case of missing phrases, or do we consider multiple
  possibilities for one phrase, even if it’s already in the table?
\end{itemize}

The quality of the phrase table depends upon the size of the data---an issue we
discuss with larger phrase tables in section~\ref{sec:exp:large-pts}---


% MD: we should give names to each model, to easily refer to them throughout the paper and in tables (IUCL1 and IUCL2 are the official submissions, but for other models we may want more descriptive names)
% alexr: the models are basically the same -- we just changed the phrase tables for the English/German setup on the second run. I'll have to check the differences exactly, but there was some problem with the phrase tables for the first run -- maybe we didn't run the whole Moses pipeline appropriately and tokenization/truecasing was messed up. But from an algorithmic perspective, they're the same.

% MD: Okay, that makes sense - do we just concatenate the data sources to derive a phrase table for each language pair?  Or is it just Europarl for this phase?

\subsection{Scoring Candidate Translations via a L2 Language Model}
\label{sec:l2model}

%%%%CL: added the following, might not be accurate!! 
To examine how well a phrase fits into an L2 context, a quick and intuitive method is to use an N-gram language model (LM), built from the L2, and rank the candidate phrases based on their LM scores. With a large vocabulary, the construction and query of an N-gram language model is potentially very time-consuming, and thus we use the KenLM Language Model Toolkit for efficiency \cite{heafield:kenlm:11}.

%%%%CL: @Alex, How many n-grams did we use, up to 3-gram, 4-gram,5-gram?

\subsection{Scoring Candidate Translations via Dependency-Based Word Similarity}
\label{sec:dependencySIM}

\paragraph{Definition} The candidate ranking based on the N-gram language model is based on very shallow information. We can also rank the candidate phrases based on how well each of the components fits into the L2 context based on syntactic information. In this case, the fitness is measured in terms of dependency-based word similarity based on dependency triples consisting of the the head, the dependent, and the dependency label. We slightly adapted the word similarity measure by \newcite{lin:98}:\marginpar{SK: Can, please check comment}
%%SK: From the text, it is not clear whether this is the original or the modified. Could you also add a sentence saying what the modification is?

\begin{equation}
SIM(w_1,w_2) = \frac{2 \prod c(h,d,l)} 
{c(h,-,l) + c(-,d,l)}
\end{equation}
\marginpar{SK: how do w1 and w2 relate to h,d,l?}

%\marginpar{MD: this said ``L1 corpus'', but I think we mean ``L2''}
\noindent
where $c(h,d,l)$ is the frequency that a particular $(head, dependent, label)$ dependency triple occurs in the L2 corpus. $c(h,-,l)$ is the frequency that a word occurs as a head in a dependency labeled $l$ with any dependent. $c(-,d,l)$ is the frequency that a word occurs as a dependent in a dependency labeled $l$  with any head. 

%\begin{itemize}
%\marginpar{MD: I think we're missing the step of comparing a word (e.g.,  \textit{eat} with its context, i.e., $fit(w_i) = \sum_{w_j}  sim(w_i,w_j)$}
%\end{itemize}

The fitness of a phrase is the average word similarity over all its components. For example the fitness of the phrase "eat with chopsticks" would be computed as:

\begin{align}
& fit(\mbox{eat with chopsticks}) = \nonumber \\
 & \qquad \frac{SIM(eat) + SIM(with) + SIM(chopsticks)}{3}
\end{align}

Since we consider the heads and dependents of a target phrase component, these may be situated  inside or outside the phrase. Both cases are included in our calculation, thus enabling us to consider a more variable, syntactically determined local context of the phrase.
By basing the calculation on a  single words's head and dependent, we focus on  tightly associated words and thus avoid data sparseness issues.

\paragraph{Back-Off}
Lexical-based dependency triples suffer from data sparsity, so in addition to computing the lexical fitness of a phrase, we also calculate the POS fitness. For example, the POS fitness of ``eat with chopsticks" would be computed as follows:

\begin{align}
& fit(\mbox{eat/VBG with/IN chopsticks/NNS}) = \nonumber \\
 & \qquad \frac{SIM(\mbox{VBG}) + SIM(\mbox{IN}) + SIM(\mbox{NNS})}{3} 
\end{align}

\paragraph{Storing and Caching}
The large vocabulary and huge number of combinations of  our head/dependent/label" triples poses an efficiency problem when querying the dependency-based word similarity values. Thus, we stored the dependency triples in a database with a Python programming interface (sqlite3) and built database indexes on the frequent query types. However, for frequently searched dependency triples, re-querying the database is still inefficient. Thus we built a query cache to store the recently queries triples. Using the database and cache significantly speeds up our system. 


\paragraph{Combining SIM and LM}
During training time, we obtained dependency-based word similarity statistics for all target languages using corresponding dependency triples. During testing time, the word similarity value is queried and combined with the N-gram language model to rank the candidate phrase. 
\marginpar{SK: how???}


%More specifically, 
%- MATE parser
%- Stanford parser
%- we used: sqlite3 (for storing dependencies?)
%Two important points: 1) PMI is able to calculate semantic/collocational relationships.  We should cite work on collocational error detection.  2) We do not use just any PMI, but one which is derived from syntactic dependencies.

\subsection{Tuning Weights with MERT}
\label{sec:mert}

- ZMERT \cite{zaidan:zmert:09}
- exact set-up

\section{Experiments}
\label{sec:exp}

\subsection{Official System}
\label{sec:exp:official}

\cite{nltkbook}

\subsection{Expansion \#1: Experiments with SIM over Dependencies}
\label{sec:exp:sim}

\subsection{Expansion \#2: Experiments with Large Phrase Tables}
\label{sec:exp:large-pts}

- EU bookshop corpus
- MultiUN corpus
- you can find so many corpora on Opus!! \cite{tiedemann:12}

\section{Conclusion} 

% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{semevaliu}


\end{document}
