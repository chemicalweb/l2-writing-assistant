%
% File twocolumn.tex
%
%%
%% Based on the style files for *SEM-2014, which were, in turn,
%% Based on the style files for COLING-2014, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn,
%% based on the style files for ACL-2010, which were, in turn,
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{semeval2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand{\wsname}{SemEval-2014}
\newcommand{\submissionpage}{\url{http://alt.qcri.org/semeval2014/index.php?id=cfp}}
\newcommand{\filename}{semeval2014}
\newcommand{\contact}{pnakov qf.org.qa}

\title{IUCL: Combining Several Information Sources for SemEval Task 5}

\author{Alex Rudnick, Levi King, Can Liu, Markus Dickinson, Sandra K\"ubler  \\
  Indiana University \\
  Bloomington, IN, USA \\
  {\tt \{alexr,leviking,liucan,md7,skuebler\}@indiana.edu} }

\date{}

\begin{document}
\maketitle
\begin{abstract}
  We describe the Indiana University system for SemEval Task 5.
%  offering an L2 translation for an L1 fragment.  
  The system is based on combining different information sources to
  arrive at a final L2 guess, incorporating: phrase tables, an L2
  language model, and collocational tendencies.  We also consider
  different data sources.
 \end{abstract}


\section{Introduction}

\blfootnote{
    %
    % for review submission
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % Place licence statement here for the camera-ready version, see
    % Section~\ref{licence} of the instructions for preparing a
    % manuscript.
    %
    % % final paper: en-uk version (to license, a licence)
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % This work is licensed under a Creative Commons 
    % Attribution 4.0 International Licence.
    % Page numbers and proceedings footer are added by
    % the organisers.
    % Licence details:
    % \url{http://creativecommons.org/licenses/by/4.0/}
    % 
    % % final paper: en-us version (to licence, a license)
    %
    \hspace{-0.65cm}  % space normally used by the marker
    This work is licenced under a Creative Commons 
    Attribution 4.0 International License.
    Page numbers and proceedings footer are added by
    the organizers.
    License details:
    \url{http://creativecommons.org/licenses/by/4.0/}
}

The task of translating an L1 fragment occurring in the midst of an L2 sentence is one in which a phrase occurs in an already-rich target language (L2) context; this makes the task quite different from translation in the general case. For this reason, we decided to approach the problem by combining standard Machine Translation technology with target language information,
% allows one to rely more heavily on target information, 
such as contextual relationships. 
% We use a log-linear model ... 
This is broken down into various steps: 1) constructing candidate translations for the L1 fragment, including weights for the likelihood of each translation; 2) scoring candidate translations via a language model of the L2; 3) scoring candidate translations via dependency-driven word similarity measure \cite{lin:98} ( in this paper we call it SIM )
%pointwise mutual information (PMI); 
and 4) combining the scores from 1)-3) via minimized error rate training (MERT), to arrive at a final solution.  Step 1) models transfer knowledge between the L1 and L2; step 2) models facts about the L2 grammar, i.e., what translations fit well into the local context; step 3) models collocational and semantic tendencies of the L2; and step 4)  gives different weights to each of the three sources of information.  Although we did not finish step 3) in time for the official results, we report it here, as it represents the most novel aspect of the system -- namely, the exploitation of the rich L2 context -- and it results in our team's best system.  In general, our system is fully language-independent, with accuracy varying due to the size of data sources and quality of input technology (e.g., syntactic parse accuracy).

\section{Data Sources}
%%%% CL: added the following, not sure if correct.
The data sources serves two major steps of our system: 1) For L2 candidate generation we used Europarl, Bebelnet 2) For candidate ranking using L1 context we used Wikipedia, Google Books Syntactic N-grams. The data sources are described below:\\

\paragraph{Europarl}  %(MD: for phrase alignment / candidate generation?)
%%%% CL: Added the following sentence not sure it is correct.
The Europarl Parallel Corpus is a corpus of proceedings of the European Parliament. It is a good resource for constructing translation phrase tables because the corpus contains 21 European Languages, and it is aligned sentence by sentence. From this corpus, we built phrase tables for English-Spanish, English-German, French-English, Dutch-English.\\
%%% CL: Alex, is that right?

\paragraph{Babelnet} %(MD: can’t remember what this was for …)
%%% CL: added not sure if it correct. 
In the case our constructed phrase-tables do not contain a translation for a source phrase, we need to split up the source phrase into smaller components and look up each components from Babelnet. \\
- we used: University of Pisa wikipedia extractor
- MD: make clear which data sources were for which parts (quick point-aheads should be good)

\paragraph{Wikipedia} %(MD: for dependency/PMI calculation?) Levi:
For German and Spanish, we used recent Wikipedia dumps\marginpar{LK: Alex may have the exact info for the dumps if needed}. To save time during parsing, sentences longer than 25 words were removed. The remaining sentences were POS tagged and dependency parsed using Mate Parser and pre-trained models \cite{bohnet:10,bohnet:kuhn:12,seeker:kuhn:13}. To keep our English Wikipedia data set comparable in size to the German and Spanish sets, we chose an older (2006), smaller dump. Long sentences were removed, and the remaining sentences were POS tagged and dependency parsed using the pre-trained Stanford Parser \cite{klein:manning:03,marneffe:maccartney:ea:06}. The resulting sizes of the data sets are (roughly): German: 389M words, 28M sentences; Spanish: 147M words, 12M sentences; English: 253M words, 15M sentences. Dependencies extracted from these parsed data sets served as training for the SIM system described in section \ref{sec:dependencySIM}.
%%% CL: training changed to data source, because it is not strictly training?

\paragraph{Google Books Syntactic N-grams}
\marginpar{\cite{goldberg:orwant:13} says this contains 919M ``items''; = dependency types? Taken from 3,473,595 books.}
For English, we also trained a SIM system on the arcs dataset of the Google Books Syntactic N-Grams \cite{goldberg:orwant:13}.%[[
%--Levi: Here are more exact numbers, if needed:
%German Wikipedia:
%number_of_sents: 27,657,877
%number_of_words: 388,635,816
%avg wds per sent: 14.05
%Spanish Wikipedia:
%number_of_sents: 11,577,130
%number_of_words: 146,850,355
%avg wds per sent: 12.68
%English Wikipedia:
%number_of_sents: 14,547,754
%number_of_words: 252,934,861
%avg wds per sent: 17.39
%]]

\section{Our  System}

\subsection{Constructing Candidate Translations}

- GIZA++ and Moses for phrase table extraction

Given that this is essentially a local translation problem, we use as our starting point a phrase table constructed from parallel text, weighted with probabilities.  We use GIZA++ \cite{och:ney:00} and Moses \cite{koehn:hoang:ea:07} to construct the phrase tables.  The quality of the phrase table depends upon the size of the data--an issue we discuss with larger phrase tables in section XX--but in the case of missing phrases from the table, we back off to subphrases in the following way: ...

MD: 1. What is the exact back-off procedure? 2. Do we back off only in the case of missing phrases, or do we consider multiple possibilities for one phrase, even if it’s already in the table?

MD: we should give names to each model, to easily refer to them throughout the paper and in tables (IUCL1 and IUCL2 are the official submissions, but for other models we may want more descriptive names)
alexr: the models are basically the same -- we just changed the phrase tables for the English/German setup on the second run. I'll have to check the differences exactly, but there was some problem with the phrase tables for the first run -- maybe we didn't run the whole Moses pipeline appropriately and tokenization/truecasing was messed up. But from an algorithmic perspective, they're the same.

MD: Okay, that makes sense - do we just concatenate the data sources to derive a phrase table for each language pair?  Or is it just Europarl for this phase?

\subsection{Scoring Candidate Translations via a L2 Language Model}
%%%%CL: added the following, might not be accurate!! 
To examine how well a phrase fit into a L1 context, the most intuitive method is to use a N-gram model built from the L1 and rank the candidate phrases using N-gram scores. With a large vocabulary, the construction and query of a N-gram language model could be very time consuming, we used KenLM Language Model Toolkit for efficiency \cite{heafield:kenlm:11}. 

%%%%CL: @Alex, How many n-grams did we use, up to 3-gram, 4-gram,5-gram?

\subsection{Scoring Candidate Translations via Dependency-Based SIM}
\label{sec:dependencyPMI}

- MATE parser
- Stanford parser
- we used: sqlite3 (for storing dependencies?)

Two important points: 1) PMI is able to calculate semantic/collocational relationships.  We should cite work on collocational error detection.  2) We do not use just any PMI, but one which is derived from syntactic dependencies.

\subsection{Tuning Weights with MERT}
- ZMERT \cite{zaidan:zmert:09}
- exact set-up

\section{Experiments}

\subsection{Official System}
\cite{nltkbook}

\subsection{Expansion \#1: Experiments with PMI over Dependencies}

\subsection{Expansion \#2: Experiments with Large Phrase Tables}

- EU bookshop corpus
- MultiUN corpus
- you can find so many corpora on Opus!! \cite{tiedemann:12}

\section{Conclusion} 

% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{semevaliu}


\end{document}
