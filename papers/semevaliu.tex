\documentclass[11pt]{article}
\usepackage{semeval2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym,amsmath}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand{\wsname}{SemEval-2014}
\newcommand{\submissionpage}{\url{http://alt.qcri.org/semeval2014/index.php?id=cfp}}
\newcommand{\filename}{semeval2014}
\newcommand{\contact}{pnakov qf.org.qa}

\title{IUCL: Combining Information Sources for SemEval Task 5}

\author{Alex Rudnick, Levi King, Can Liu, Markus Dickinson, Sandra K\"ubler  \\
  Indiana University \\
  Bloomington, IN, USA \\
  {\tt \{alexr,leviking,liucan,md7,skuebler\}@indiana.edu} }

\date{}

\begin{document}
\maketitle
\begin{abstract}
Here we describe the Indiana University system for SemEval Task 5, the L2
writing assistant task, as well as some extensions to the system that were
completed after the main evaluation. Our team submitted translations for all
four language pairs in the evaluation, yielding the top scores for
English-German.  The system is based on combining different information sources
to arrive at a final L2 translation for a given L1 text fragment, incorporating
phrase tables extracted from bitexts, an L2 language model, a multilingual
dictionary, and dependency-based collocational models derived from large
samples of target-language text.
\end{abstract}


\section{Introduction}

\blfootnote{
    %
    % for review submission
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % Place licence statement here for the camera-ready version, see
    % Section~\ref{licence} of the instructions for preparing a
    % manuscript.
    %
    % % final paper: en-uk version (to license, a licence)
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % This work is licensed under a Creative Commons 
    % Attribution 4.0 International Licence.
    % Page numbers and proceedings footer are added by
    % the organisers.
    % Licence details:
    % \url{http://creativecommons.org/licenses/by/4.0/}
    % 
    % % final paper: en-us version (to licence, a license)
    %
    \hspace{-0.65cm}  % space normally used by the marker
    This work is licenced under a Creative Commons 
    Attribution 4.0 International License.
    Page numbers and proceedings footer are added by
    the organizers.
    License details:
    \url{http://creativecommons.org/licenses/by/4.0/}
}

The task of translating an L1 fragment occurring in the midst of an L2 sentence is one in which a phrase occurs in an already-rich target language (L2) context; this makes the task quite different from translation in the general case. 
For this reason, we decided to approach the problem by combining standard
statistical machine translation approaches with target language information,
% allows one to rely more heavily on target information, 
such as collocational relationships between tokens in L2 context and the
proposed fragment translations.
% We use a log-linear model ... 
This is broken down into various steps: 1) constructing candidate translations for the L1 fragment, including weights for the likelihood of each translation; 2) scoring candidate translations via a language model of the L2; 3) scoring candidate translations via dependency-driven word similarity measure \cite{lin:98} (which we call \textit{SIM})
%pointwise mutual information (PMI); 
and 4) combining the scores from 1)-3) via minimized error rate training
(MERT), to arrive at a final n-best list.  Step 1) models transfer knowledge
between the L1 and L2; step 2) models facts about the L2 grammar, i.e., what
translations fit well into the local context; step 3) models collocational and
semantic tendencies of the L2; and step 4)  gives different weights to each of
the three sources of information.  Although we did not finish step 3) in time
for the official results, we report it here, as it represents the most novel
aspect of the system -- namely, steps towards the exploitation of the rich L2
context. In general, our approach is language-independent, with accuracy
varying due to the size of data sources and quality of input technology (e.g.,
syntactic parse accuracy). More features could easily be added to the
log-linear model framework, and further explorations of ways to make use of
target-language knowledge could be promising.

\section{Data Sources}
%%%% CL: added the following, not sure if correct.
The data sources serve two major steps of our system: for L2 candidate generation, we use Europarl and BabelNet; and for candidate ranking based on L2 context, we use Wikipedia and the Google Books Syntactic N-grams. 

\paragraph{Europarl}  %(MD: for phrase alignment / candidate generation?)
%%%% CL: Added the following sentence not sure it is correct.
The Europarl Parallel Corpus (Europarl, v7) \cite{koehn:05} is a corpus of proceedings of the European Parliament, containing 21 European languages with sentence alignments.
%It is a good resource for constructing translation phrase tables because the corpus contains 21 European Languages, and it is aligned sentence by sentence. 
From this corpus, we build phrase tables for English-Spanish, English-German, French-English, Dutch-English.
%%% CL: Alex, is that right?

\paragraph{BabelNet} %(MD: can’t remember what this was for …)
%%% CL: added not sure if it correct. 
%\marginpar{MD: check version number}

In the cases where the constructed phrase tables do not contain a translation
for a source phrase, we will need to back off to smaller phrases and find
candidate translations for these components.  To better handle sparsity, we
extend look-up using the multilingual dictionary BabelNet, v2.5
\cite{Navigli:Ponzetto:12} as a way to find translation candidates.

\paragraph{Wikipedia} %(MD: for dependency/PMI calculation?) Levi:

For German and Spanish, we use recent Wikipedia dumps, which were converted to
plain text with the Wikipedia Extractor tool.\footnote{\url{http://medialab.di.unipi.it/wiki/Wikipedia_Extractor}} To save time during parsing, sentences longer than 25 words are removed. The remaining sentences are POS-tagged and dependency parsed using Mate Parser with its pre-trained models \cite{bohnet:10,bohnet:kuhn:12,seeker:kuhn:13}. To keep our English Wikipedia dataset comparable in size to the German and Spanish sets, we choose an older (2006), smaller dump. Long sentences are removed, and the remaining sentences are POS-tagged and dependency parsed using the pre-trained Stanford Parser \cite{klein:manning:03,marneffe:maccartney:ea:06}. The resulting sizes of the datasets are (roughly): German: 389M words, 28M sentences; Spanish: 147M words, 12M sentences; English: 253M words, 15M sentences. Dependencies extracted from these parsed datasets serve as training for the SIM system described in section \ref{sec:dependencySIM}.
%%% CL: training changed to data source, because it is not strictly training?

\paragraph{Google Books Syntactic N-grams}
% \marginpar{\cite{goldberg:orwant:13} says this contains 919M ``items''; = dependency types? Taken from 3,473,595 books.}

%%%%CL THUR: I am wondering SIM system is not enough for the reader to understand, added something.
For English, we also obtained dependency relationships for our word similarity
statistics using the arcs dataset of the Google Books Syntactic N-Grams
\cite{goldberg:orwant:13}, which has 919M items, each of which is a small
``syntactic n-gram", a term Goldberg and Orwant use to describe short
dependency chains, which may contain several tokens. This data set does not
contain the actual parses of books from the Google Books corpus, but counts of
these dependency chains. We converted the longer chains into their component
$(head,dependent,label)$ triples and then collated these triples into counts,
also for use in the SIM system.

%[[
%--Levi: Here are more exact numbers, if needed:
%German Wikipedia:
%number_of_sents: 27,657,877
%number_of_words: 388,635,816
%avg wds per sent: 14.05
%Spanish Wikipedia:
%number_of_sents: 11,577,130
%number_of_words: 146,850,355
%avg wds per sent: 12.68
%English Wikipedia:
%number_of_sents: 14,547,754
%number_of_words: 252,934,861
%avg wds per sent: 17.39
%]]

\section{Our System}
\label{sec:system}

As previously mentioned, at run-time our system decomposes the fragment
translation task into two parts: generating a number of possible candidate
translations, and then scoring and ranking them in the target-language context,
such that best-scoring translations can be returned.

\subsection{Constructing Candidate Translations}
\label{sec:candidates}

As a starting point, we use phrase tables constructed in typical SMT fashion,
using the training scripts packaged with Moses \cite{koehn:hoang:ea:07}, which
preprocess the given bitext, find word alignments with GIZA++ \cite{och:ney:00}
and then extract phrases with the \texttt{grow-diag-final-and} heuristic. 

At translation time, we look for the given source-language phrase in the phrase
table, and if it is found, we take all translations of that phrase as our
candidates.
%MD: If space needed, combine above two paragraphs.

When trying to translate a source-language phrase that is not found in the
phrase table, we try to construct a ``synthetic phrase" out of the available
components. This is done by listing, combinatorially, all ways to decompose the
input phrase into sub-phrases of at least one token long. Then for each
decomposition of the input phrase such that all of its components can be found
in the phrase table, we generate a translation by concatenating their
target-language sides. This approach naively assumes that generating valid L2
text requires no reordering of the components. Also, since there are $2^{n-1}$
possible ways to split an $n$-token phrase into sub-sequences (\textit{i.e.},
each token is either the first token in a new sub-sequence, or it is not), we
perform some heuristic pruning at this step, taking only the first 100
decompositions, preferring those built from longer phrase-table entries. Every
phrase in the phrase table, including these synthetic phrases, has both a
``direct" and ``inverse" probability score; for synthetic phrases, we produce
probability estimates by taking the product of probabilities for the
individual components.

In the case that an individual word cannot be found in the phrase table, the
system attempts to look up the word in BabelNet, estimating the probabilities
as uniformly distributed over the available entries in BabelNet. Thus,
synthetic phrase table entries can be constructed by combining phrases found in
the training data and words available in BabelNet.

For the evaluation, in cases where an L1 phrase contained words that were
neither in our training data nor BabelNet (and thus were simply
out-of-vocabulary for our system), we took the first translation for that
phrase, without regard to context, from Google Translate, through the
semi-automated Google Docs interface. This approach is not particularly
scalable or reproducible, but simulates what a user might do in such a
situation.

%%%% CL THUR:  this paragraph is not complete.

% MD: we should give names to each model, to easily refer to them throughout the paper and in tables (IUCL1 and IUCL2 are the official submissions, but for other models we may want more descriptive names)
% alexr: the models are basically the same -- we just changed the phrase tables for the English/German setup on the second run. I'll have to check the differences exactly, but there was some problem with the phrase tables for the first run -- maybe we didn't run the whole Moses pipeline appropriately and tokenization/truecasing was messed up. But from an algorithmic perspective, they're the same.
% MD: Okay, that makes sense - do we just concatenate the data sources to derive a phrase table for each language pair?  Or is it just Europarl for this phase?

\subsection{Scoring Candidate Translations via a L2 Language Model}
\label{sec:l2model}

%%%%CL: added the following, might not be accurate!! 
To examine how well a phrase fits into an L2 context, a quick and intuitive
method is to use an n-gram language model (LM) trained on a large sample of
target-language text, scoring candidate phrases based on their LM
probabilities.
Constructing and querying a large language model is potentially very
time-consuming, so here we use the KenLM Language Model Toolkit and its Python
interface for efficiency \cite{heafield:kenlm:11}. Here our models were trained
on the Wikipedia text mentioned previously, with KenLM set to 5-grams and the
default settings.

\subsection{Scoring Candidate Translations via Dependency-Based Word Similarity}
\label{sec:dependencySIM}

\paragraph{Definition} The candidate ranking based on the N-gram language model is based on very shallow information. We can also rank the candidate phrases based on how well each of the components fits into the L2 context using syntactic information. In this case, the fitness is measured in terms of dependency-based word similarity computed from dependency triples consisting of the the head, the dependent, and the dependency label. We slightly adapted the word similarity measure by \newcite{lin:98}:
%%SK: From the text, it is not clear whether this is the original or the modified. Could you also add a sentence saying what the modification is?
%%%% CL THURS: I added explaination at line 196.
\begin{equation}
SIM(w_1,w_2) = \frac{2 \prod c(h,d,l)} 
{c(h,-,l) + c(-,d,l)}
\end{equation}
%\marginpar{SK: how do w1 and w2 relate to h,d,l?}
%%%% CL THURS:  I added a sentence at line 194

%\marginpar{MD: this said ``L1 corpus'', but I think we mean ``L2''}
%%% CL THURS, thanks Markus, that's right!
\noindent
where $h=w_1$ and $d=w_2$ and 
$c(h,d,l)$ is the frequency that a particular $(head, dependent, label)$ dependency triple occurs in the L2 corpus. $c(h,-,l)$ is the frequency that a word occurs as a head in a dependency labeled $l$ with any dependent. $c(-,d,l)$ is the frequency that a word occurs as a dependent in a dependency labeled $l$  with any head. 
In \cite{lin:98}'s measure, the numerator is defined as the information of all dependency features that $w_1$ and $w_2$ share, computed as the negative sum of the log probability of each dependency feature. Similarly, the denominator is computed as the sum of information of dependency features for $w_1$ and $w_2$. \\
%\begin{itemize}
%\marginpar{MD: I think we're missing the step of comparing a word (e.g.,  \textit{eat} with its context, i.e., $fit(w_i) = \sum_{w_j}  sim(w_i,w_j)$}
%%%% CL THURS: added a sentence at line 202~205.
%\end{itemize}

To compute the fitness of a word $w_i$ to its context, we consider a set $D$ of all words that are directly dependency-related to $w_i$. The fitness of $w_i$ is thus computed as:\\
\begin{equation}
FIT(w_i) =  \frac {  \sum_{w_j}^{D} sim(w_i,w_j) } {  |D|}
\end{equation}

The fitness of a phrase is the average word similarity over all its components. For example the fitness of the phrase ``eat with chopsticks'' would be computed as:

\begin{align}
& FIT(\mbox{eat with chopsticks}) = \nonumber \\
 & \qquad \frac{FIT(\mbox{eat}) + FIT(\mbox{with}) + FIT(\mbox{chopsticks})}{3}
\end{align}

Since we consider the heads and dependents of a target phrase component, these may be situated  inside or outside the phrase. Both cases are included in our calculation, thus enabling us to consider a more variable, syntactically-determined local context of the phrase.
By basing the calculation on a single word's head and dependents, we focus on tightly associated words and thus avoid data sparseness issues that we might get from an n-gram context.

\paragraph{Back-Off}
Lexical-based dependency triples suffer from data sparsity, so in addition to computing the lexical fitness of a phrase, we also calculate the POS fitness. For example, the POS fitness of ``eat with chopsticks'' would be computed as follows:

\begin{align}
& FIT(\mbox{eat/VBG with/IN chopsticks/NNS}) = \nonumber \\
 & \qquad \frac{FIT(\mbox{VBG}) + FIT(\mbox{IN}) + FIT(\mbox{NNS})}{3} 
\end{align}


%%%% CL THURS: myself is a little confused where we should use FIT or SIM for defining the equations above. I am voting for FIT, because FIT meant to be the fitness of the context, which can involve more than one word. where SIM is defined first for just two words. 

\paragraph{Storing and Caching}
The large vocabulary and huge number of combinations of our $(head, dependent, label)$ triples poses an efficiency problem when querying the dependency-based word similarity values. Thus, we stored the dependency triples in a database with a Python programming interface (SQLite3) and built database indices on the frequent query types. However, for frequently searched dependency triples, re-querying the database is still inefficient. Thus we built a query cache to store the recently-queried triples. Using the database and cache significantly speeds up our system. 

\begin{figure}[t]
  \begin{center}
  \begin{tabular}{|r|c|c|c|c|}
    \hline
    system & acc      & wordacc  & oofacc & oofwordacc \\
    \hline
    run2  &  0.665 & 0.722  &  0.806  & 0.857 \\
    SIM    &  0.647 & 0.706 & 0.800 & 0.852 \\
    best   &  0.657   & 0.717   & 0.834 & 0.868    \\
    \hline
  \end{tabular}
  \end{center}
\caption{Scores on the test set for English-German; here best is CNRC-run1.}
\label{fig:theresults-en-de}
\end{figure}

\begin{figure}[t]
  \begin{center}
  \begin{tabular}{|r|c|c|c|c|}
    \hline
    system & acc      & wordacc  & oofacc & oofwordacc \\
    \hline
    run2  &  0.633 & 0.72 & 0.781 & 0.847 \\
    SIM    &  0.359 &  0.482 & 0.462 & 0.607 \\
    best   &  0.755 & 0.827   & 0.920  & 0.944 \\
    \hline
  \end{tabular}
  \end{center}
\caption{Scores on the test set for English-Spanish; here best is UEdin-run2.}
\label{fig:theresults-en-es}
\end{figure}

\begin{figure}[t]
  \begin{center}
  \begin{tabular}{|r|c|c|c|c|}
    \hline
    system & acc      & wordacc  & oofacc & oofwordacc \\
    \hline
    run2  & 0.545  & 0.682 & 0.691 & 0.800 \\
    SIM        &  0.549 & 0.687 & 0.693 & 0.800 \\
    best & 0.733 & 0.824 & 0.905 & 0.938 \\
    \hline
  \end{tabular}
  \end{center}
\caption{Scores on the test set for French-English; here best is UEdin-run1.}
\label{fig:theresults-fr-en}
\end{figure}

\begin{figure}[t]
  \begin{center}
  \begin{tabular}{|r|c|c|c|c|}
    \hline
    system & acc      & wordacc  & oofacc & oofwordacc \\
    \hline
    run2        &  0.544      &  0.679  & 0.634   & 0.753    \\
    SIM              &  0.540      &  0.676  & 0.635   & 0.753    \\
    best       &  0.733      &  0.811  & 99      & 57       \\
    \hline
  \end{tabular}
  \end{center}
\caption{Scores on the test set for Dutch-English; here best is UEdin-run1.}
\label{fig:theresults-nl-en}
\end{figure}


\paragraph{Combining SIM and LM}
%During training time, we obtained dependency-based word similarity statistics for all target languages using corresponding dependency triples. During testing time, the word similarity value is queried and combined with the N-gram language model to rank the candidate phrase. 
Note that we only store dependency triples and their corresponding counts; the dependency-based similarity value is calculated on the fly during training and testing. For each candidate phrase, the scores from LM and SIM are combined using a weight vector $v$, which is optimized during training time using MERT. 
%\marginpar{SK: how???}
%%%%% CL THURS:  does the above paragraph make more sense?

%More specifically, 
%- MATE parser
%- Stanford parser
%- we used: sqlite3 (for storing dependencies?)
%Two important points: 1) PMI is able to calculate semantic/collocational relationships.  We should cite work on collocational error detection.  2) We do not use just any PMI, but one which is derived from syntactic dependencies.

\subsection{Tuning Weights with MERT}
\label{sec:mert}

In order to rank the various candidate translations, we must combine the
different sources of information in some way.  Here we use a familiar
log-linear model, taking each score -- the direct and inverse translation
probabilities, the LM probability, and the surface and POS SIM scores -- take
the log of each of these, and then produce a weighted sum. Since the original
scores are either probabilities or probability-like (in the range $[0,1]$),
their logs are negative numbers, and at translation time, we return the
translation (or $n$-best) with the highest (least negative) score.

This leaves us with the question of how to set the weights for the log-linear
model; in this work we use the ZMERT package \cite{zaidan:zmert:09}, which
implements the MERT optimization algorithm \cite{och:2003:ACL}, iteratively
tuning the feature weights by repeatedly requesting $n$-best lists from the
system. Here we used ZMERT with its default settings, optimizing our system's
BLEU scores on the provided development set.

\section{Experiments}
\label{sec:exp}

In the above figures, we show the scores on this year's test set for running
the two variations of our system: \emph{run2}, the version we submitted for the
evaluation, without the SIM extensions, and \emph{SIM}, with the extensions
enabled. For comparison, we also include the best (or for English-German,
next-best) submitted system.
We see here that the use of the SIM features did not improve the performance of
the base system, and in the case of English-Spanish caused significant
degradation, which is as of yet unexplained, though we suspect difficulties
parsing the Spanish test set, as for all of the other language pairs, the
effects of adding SIM features were small.

\section{Conclusion} 
Here we have described our entry in the initial running of the ``L2 Writing
Assistant" task and explained some possible extensions to our base log-linear
model system.

In developing the SIM extensions, we faced some interesting software
engineering challenges, and can now produce large databases of dependency
relationship counts for various languages. Unfortunately, these extensions have
not yet led to improvements in performance on this particular task. The
databases themselves seem at least intuitively promising, capturing interesting
information about common usage patterns of the target language.
Finding a way to make use of this information involve
computing some measure, or perhaps the insights captured by SIM are covered
effectively by the language model.

We look forward to future developments around this task and associated
applications in helping language learners communicate effectively.

\clearpage
% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{semevaliu}

\end{document}
